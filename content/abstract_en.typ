Learning Management Systems (LMSs) like Artemis now are able to assess student submissions in seconds automatically, yet the feedback they return is largely uniform, overlooking differences in students' prior knowledge, engagement patterns, and preferred learning styles—factors that research shows are critical for effective learning. This mismatch between scale and individual relevance limits students' ability to bridge the gap between their current performance and course goals. This thesis addresses this deficiency by enabling LMSs to provide individualized feedback to students. 

We present a profile-aware feedback architecture for Artemis and Athena. The main contribution is a learner-profiling framework that aggregates competencies, submission history, and explicit feedback preferences, and then injects that profile into a large-language-model (LLM) pipeline to generate submission-specific guidance for online exercises.

The work proceeded iteratively. In each cycle, we extended the pipeline—first by defining which learning-style dimensions and competencies to track, next by structuring those competencies, and finally by tailoring the visual feedback presentation. User interviews with students revealed that personalized feedback is helpful and aligned with their preferences.