Learning Management Systems like Artemis now are able to assess student submissions in seconds automatically, yet the feedback they return is largely uniform, overlooking differences in students' prior knowledge, engagement patterns, and preferred learning styles—factors that research shows are critical for effective learning. This mismatch between scale and individual relevance limits students' ability to close the gap between their current performance and course goals. This thesis aims to close this gap by providing individualized feedback to students. 

We present a profile-aware feedback architecture for Artemis and Athena. The main contribution is a learner-profiling framework that aggregates competencies, submission history, and explicit feedback preferences, and then injects that profile into a large-language-model (LLM) pipeline to generate submission-specific guidance for programming, text, and modelling exercises.

The work proceeded iteratively. In each cycle, we extended the pipeline—first by defining which learning-style dimensions and competencies to track, next by structuring those competencies, and finally by tailoring the visual feedback presentation. We ran two evaluation studies with students, using interview-style sessions to gather qualitative insights alongside timing and accuracy measurements. 